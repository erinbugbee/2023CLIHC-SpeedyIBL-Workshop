---
title: "Applications of Instance-Based Learning Theory: Using the SpeedyIBL Library to Construct Computational Models"
author:
  - name: Erin H. Bugbee
    affiliation: Carnegie Mellon University
  - name: Thuy Ngoc Nguyen
    affiliation: University of Dayton
  - name: Cleotilde Gonzalez
    affiliation: Carnegie Mellon University
subtitle: "CLIHC 2023, Puebla, Mexico"
format:
    revealjs:
#      incremental: true   
      slide-number: true
      theme: custom_cmu.scss
editor: visual
bibliography: references.bib
csl: apa-single-spaced.csl
slide-level: 3
execute:
  echo: true
engine: knitr
embed-resources: false
#title-slide-attributes: 
#  background-image: "https://www.cmu.edu/dietrich/sds/ddmlab/imgs/ddmlablogo.png"
---

## Agenda

1.  Introduction to Instance-Based Learning Theory
2.  Introduction to the SpeedyIBL Library with a Binary Choice Task Example
3.  Building an IBL Model for the Iowa Gambling Task
4.  Conclusion and Additional Applications

# Introduction to Instance-Based Learning Theory

## Complex, Dynamic Decision Making (DDM)

-   Dynamic and sequential multi-attribute decisions that are interdependent over-time

-   High uncertainty and change at different time scales 

-   Dynamic allocation of limited resources (time, drones, people)

-   There is not enough time for considering all alternatives before making a choice

-   Human cognitive abilities (attention, memory) are limited

## Machine AI and Complex Societal DDM Problems

-   Machine AI is supporting complex DDM problems in multiple ways --- aggregating data, detecting patterns, and forecasting and projecting events based on data
-   But often there is not enough data, particularly regarding high-level decisions
    -   Requires a deep understanding of context, consideration of multiple and possibly conflicting factors and their consequences

    -   The incorporation of ethical and moral trade-offs

------------------------------------------------------------------------

<br><br> <br><br>

> The ultimate **responsibility for complex decisions lies, and *will continue to lie*, with humans.**

## Why Should Machines Learn?

::: columns
::: {.column width="20%"}
![Herbert A. Simon, 1983](images/Herbert-A-Simon.jpg.webp){fig-align="center"}
:::

::: {.column width="80%"}
*Artificial intelligence has two goals.*

*First, AI is directed toward getting computers to be smart and do smart things so that human beings don't have to do them.*

*And second, AI (sometimes called cognitive simulation, or information processing psychology) is also directed at using computers to simulate human beings, so that we can **find out how humans work and perhaps can help them to be a little better in their work.***
:::
:::

## Cognitive AI

-   **Cognitively-plausible algorithms that emulate human decision making**

-   Based on cognitive theory: explain how humans make dynamic decisions, including the prediction of cognitive biases and errors in the absence of data

-   Dynamic actionable models: able to learn and adapt to changes independently to predict human decisions

-   Can represent human experience and be synchronized with human actions

-   Can become collaborators with humans in teams

## Building Human-Like Artificial Agents using IBLT

![](images/IBLT_cycle.png){fig-align="center"}

-   Demonstrate that IBLT is a GENERAL theory of Dynamic Decision Making and can replicate human decisions from experience (DfE)

## Using Microworlds in Laboratory Experiments to Study DDM

![](images/microworlds.png){fig-align="center"}

## Approach: Cognitive Architectures

::: columns
::: {.column width="20%"}
![Allen Newell, 1990](images/allen-newell.jpg.avif)
:::

::: {.column width="80%"}
**Unified Theories of Cognition**

A single system (mind) produces all aspects of human behavior

-   Representation of cognitive steps in performing a task

-   Explain how all the components of the mind worked to produce coherent cognition.
:::
:::

## ACT-R: A unified theory of cognition [@anderson1998]

::: columns
::: {.column width="40%"}
::: columns
::: {.column width="50%"}
![](images/john_anderson.png){width="200"}
:::

::: {.column width="50%"}
![](images/christian_lebiere.jpg){width="200"}
:::
:::
:::

::: {.column width="60%"}
-   Symbolic representations of Declarative and Procedural Memories 

-   Statistical/Mathematical Mechanisms for processing, accessing, and retrieving memories, and learning and adapting behavior\
:::
:::

## IBLT: Cognitive Algorithm for Dynamic Decision Making [@gonzalez2003]

::: columns
::: {.column width="50%" style="font-size: 25px;"}
-   Decisions are made by **recognizing** **similar** situations in decisions made in the past

-   **Evaluating** new actions according to the utility of past decisions

-   Mentally **exploring** the value of the alternatives sequentially

-   **Executing a choice** that has the maximum "expected utility"

-   Re-evaluating the utility of past decisions based on **feedback** from environment
:::

::: {.column width="50%"}
![](images/IBLT.png){fig-align="center"}
:::
:::

## Representation of Decisions

::: columns
::: {.column width="50%" style="font-size: 30px;"}
-   Decisions are stored over-time in memory (**Instances**) consisting of **Contextual features, Action, Outcome (Utility)**

-   For each potential Action i**nstances are Blended** accounting for similarity, to create an **expected utility**

-   The Action with the **highest Blended Value** is executed, and new instances are created.

-   The **Blended Value is updated** with an experienced outcome through **feedback**
:::

::: {.column width="50%"}
\
![](images/instance_image-01.png)
:::
:::

## IBLT Mathematical Mechanisms

ACT-R's Activation Equation

$$
\Lambda_{ik_it} = \ln{\left(\sum\limits_{t' \in T_{ik_it} }(t-t')^{-d}\right)} +
$$ $$ 
\alpha\sum\limits{j}Sim_j(f_j^k, f_j^{k_i}) +
$$ $$
\sigma\ln{\frac{1-\xi_{ik_it}}{\xi_{ik_it}}}
$$

------------------------------------------------------------------------

Probability of Retrieval

$$
P_{i k_i t} = \frac{e^{\Lambda_{ik_it}/\tau}}{\sum_{j = 1}^{n_{kt}}e^{\Lambda_{jk_jt}/\tau}}
$$

Blended Value

$$
V_{kt} = \sum_{i=1}^{n_{kt}}P_{ik_it}x_{i k_i t}
$$

Action Selection

$$
  a_l = \arg\max_{a\in A}  V_{(s_l,a),t}
$$

# Introduction to the SpeedyIBL Library with a Binary Choice Task Example

## SpeedyIBL Library [@nguyen2022]

![](images/speedyIBL.png){fig-align="center"}

## Motivation

-   An open-source, efficient library facilitates the implementation of models based on IBLT mechanisms

-   Demonstrate how to use SpeedyIBL to handle a diverse taxonomy of individual and multiagent decision-making tasks

## Demonstrations

![](images/demonstrations.gif){fig-align="center"}

## Notebook: [*Introduction_SpeedyIBL.ipynb*](https://github.com/DDM-Lab/2023CLIHC-SpeedyIBL-Workshop/blob/main/Introduction_SpeedyIBL.ipynb)

## Installing SpeedyIBL Library

::: {#cloud}
::: nonincremental
**Using online Jupyter Notebook such as [Google Colab](https://colab.research.google.com/):**
:::

```{python}
#| eval: false
pip install -U speedyibl
```
:::

![](images/install.png)

## Installing SpeedyIBL Library

::: {#local}
::: nonincremental
**Running locally:**

-   Must have Python 3 installed
-   (suggestion) Create a virtual Python Environment by running the following commands in your shell.
:::

```{python}
#| eval: false
# Use venv module by entering the following commands in your shell:
python3 -m venv env
source env/bin/activate
pip install -U speedyibl

# OR use conda commands (with Miniconda/Anaconda installed):
conda create -n myenv python=3.9
conda activate myenv
pip install -U speedyibl	
```
:::

## An IBL Agent with SpeedyIBL

### Import the class `Agent` from `speedyibl`

```{python}
from speedyibl import Agent
```

![](images/agent.png){fig-align="center"}

## Attributes (properties) of the class `Agent`

![](images/inputs.png){fig-alt="Description of inputs of the class Agent"}

### To view the list of attributes:

```{python}

help(Agent.__init__)
```

## Functions (methods) of the class `Agent`

![](images/functions.png){fig-alt="Functions of the class Agent"}

### To display the list of methods:

```{python}
dir(Agent)
vars(Agent)
```

## SpeedyIBL for Binary Choice Task

![](images/binary_choice.png){fig-align="center"}

## Notebook: [*Demonstration_BinaryChoice.ipynb*](https://github.com/DDM-Lab/2023CLIHC-SpeedyIBL-Workshop/blob/main/Demonstration_BinaryChoice.ipynb)

### The Binary Choice Task

![](images/binary_cycle.png){fig-align="center"}

-   Choose one of two options: **Option A** or **Option B**

    -   One option is **SAFE,** always yielding a fixed medium outcome (3)

    -   One option is **RISKY,** yielding a high outcome (4) with probability of 0.8, and a low outcome (0) with the complementary probability of 0.2

-   **Single-state** with **immediate** feedback task

## Create an Environment for the Binary Choice Task

![](images/environment.png){fig-align="center"}

### Defining Options

::: {#options}
::: nonincremental
-   Define a list of options for the agent to choose

```{python}
#| eval: false
options = ['A', 'B'] # A is the safe option while B is the risky one
```
:::
:::

### Defining Reward

::: {#reward}
::: nonincremental
-   Define the reward for choosing an option
:::

```{python}
#| eval: false
import random

def reward(choice):
  if choice == 'A':
    r = 3
  elif random.random() <= 0.8:
    r = 4
  else:
    r = 0
  return r
```
:::

### Putting Together in Class `BinaryChoice`

```{python}
import random 

class BinaryChoice:
  def __init__(self):
    self.options = ['A', 'B']
    
  def reward(self, choice):
    if choice == 'A':
      r = 3
    elif random.random() <= 0.8:
      r = 4
    else:
      r = 0
    return r
```

### Declare an Environment

```{python}
env = BinaryChoice()
```

### Building an Agent for the Binary Choice Task

Create an IBL agent for the binary choice task:

::: {#build}
::: nonincremental
-   Indicating a value of `default_utility`

```{python}
from speedyibl import Agent

agent = Agent(default_utility = 4.4)
```

-   Indicating values for parameters, e.g. `noise` and `decay`

```{python}
#| eval: false
agent = Agent(default_utility = 4.4, noise = 0.1, decay = 0.8)
```
:::
:::

## Interactions Between the Agent and the Task: Actions

![](images/interactions.png){fig-align="center"}

### Choose Option

-   Make the agent choose one of two options:

```{python}
choice = agent.choose(env.options)
```

## Interactions Between the Agent and the Task: Observations and Rewards

![](images/interactions_obs_rewards.png){fig-align="center"}

### Observe Reward

::: {#respond}
::: nonincremental
-   After choosing one option, the agent observes the reward and stores an instance in memory using the function `respond`:
:::

```{python}
r = env.reward(choice)
agent.respond(r)
```
:::

### View the Agent's Memory

::: {#instances}
::: nonincremental
-   To see how instances are stored in the memory
:::

```{python}
agent.instances()
```
:::

-   Example:

![](images/memory.png){fig-align="center"}

## IBL Agent's Decision Mechanisms

### Activation of Each Instance

```{python}
print(agent.CompActivation(agent.t+1, 'A'))
print(agent.CompActivation(agent.t+1, 'B'))
```

### Probability of Retrieving Each Instance

```{python}
print(agent.CompProbability(agent.t+1, "A"))
print(agent.CompProbability(agent.t+1, "B"))
```

### Blended Value for Each Option

```{python}
print(agent.CompBlended(agent.t+1, env.options))
```

### Simulating Choices for Many Rounds

-   Let the agent play 100 rounds (trials) for the binary choice task
    -   Each round includes *choosing* one option, *observing* the reward, and *storing* the instance

![](images/rounds.png){fig-align="center"}

### Simulating Choices for Many Rounds: Code

::: {#simulation}
```{python}
rounds = 100 # number of rounds (trials)
pmax = []
for i in range(rounds):
  choice = agent.choose(env.options) # choose one option from the list of options
  r = env.reward(choice) # observe the reward
  agent.respond(r) # store the instance
  pmax.append(choice == 'B')
  
print(f"P(Max): {sum(pmax)/rounds}")
```
:::

### Simulating Choices for Many Runs

-   Let's have 500 runs (500 subjects) playing 100 rounds of the task

![](images/runs.png){fig-align="center"}

### Simulating Choices for Many Runs: Code

```{python}
#| class-source: small_code
#| classes: small_code

runs = 500
rounds = 100 # number of rounds (trials)
pmax_per_run = []
for j in range(runs):
  pmax = []
  agent.reset() # clear the memory for a new run
  for i in range(rounds):
    choice = agent.choose(env.options) # choose one option from the list of options
    r = env.reward(choice) # observe the reward
    agent.respond(r) # store the instance
    pmax.append(choice == 'B')
  pmax_per_run.append(pmax)
```

### Plotting Performance (PMAX) over Rounds

```{python}
#| fig-align: "center"
#| fig-width: 2
import matplotlib.pyplot as plt
import numpy as np
plt.figure(figsize=(5, 3.5))
plt.plot(range(rounds), np.mean(pmax_per_run,axis=0), color='darkgreen',
          markersize=2, linestyle='--', label='speedyIBL')
plt.xlabel('Round')
plt.ylabel('PMAX')
plt.title('Performance')
plt.ylim(0,1)
plt.grid(True)
plt.show()
```

## Exercise 1

-   Simulate the effect of the IBL model inputs in the Binary Choice Task

    -   `default_utility` of 3, `noise` of 0.25, and `decay` of 0.5

    -   `default_utility` of 4.4, `noise` of 0.5, and `decay` of 0.8

## Exercise 2

-   The table below from [@hertwig2004] presents the proportion of maximization obtained from experimentation in **Humans**

    -   Simulate the Problem 5

    -   Compare the IBL agents' PMAX vs. Humans' PMAX (Pmax2 column)

![](images/human_data.png){fig-align="center"}

# Building an IBL Model for the Iowa Gambling Task

## Iowa Gambling Task History

-   Developed by researchers to study risky decision making [@bechara1994]

-   Widely used to study human cognition

## Iowa Gambling Task Demo

::: {#igt}
::: nonincremental
-   To play the demo of game: <https://www.psytoolkit.org/experiment-library/igt.html>

-   4 decks of cards (A, B, C, and D).

-   Participants started with a "loan" of \$2000 and were told to make a profit.
:::

![](images/iowa1.png){fig-align="center" width="300"}
:::

. . .

## Iowa Gambling Task: Choices

::: {#igt2}
::: nonincremental
-   Participants had to choose 100 cards in total, one at the time. Each time they choose a card, they get feedback about winning and/or losing some money.

-   They did not know what each card would yield in advance (i.e., a lottery).

::: columns
::: {.column width="50%"}
![](images/iowa2.png){fig-align="center" width="297"}
:::

::: {.column width="50%"}
![](images/iowa3.png){fig-align="center" width="302"}
:::
:::
:::
:::

## Task Environment

-   Decks A and B always yielded \$100

-   Decks C and D always yielded \$50

-   For each card chosen, there is a 50% chance of having to pay a penalty as well. For decks A and B, the penalty is \$250, whereas for decks C and D it is \$50.

## Notebook: [*Exercise_IowaGambling.ipynb*](https://github.com/DDM-Lab/2023CLIHC-SpeedyIBL-Workshop/blob/main/Exercise_IowaGambling.ipynb)

### Steps

1.  Install and import `speedyibl`
2.  Define an IBL agent
3.  Define list of options
4.  Choose option from list of options
5.  Store the observed reward to memory of the agent

## Steps

6.  Run the simulation and observe the plots for `default_utility=110` and `default_utility=10`
7.  Vary the noise and decay parameters and plot the `PMAX` and `AverageReward` over rounds with `default_utility=110`
8.  Run the simulation for varied parameter values and plot results

## Notebook: [*Solution_IowaGambling.ipynb*](https://github.com/DDM-Lab/2023CLIHC-SpeedyIBL-Workshop/blob/main/Solution_IowaGambling.ipynb)

# Conclusion

## Additional Applications

## Gridworld

-   A sequential decision making problem wherein a decision maker navigates through a grid by making sequential decisions about which actions to take (Up, Down, Left, Right) to search for a target and avoid obstacles

    -   Dimension of environment: 3 x 4 grid that contains an obstacle (black cell)

    -   Decision maker starts at an initial position (marked Start) and has a 25-step limit

    -   One target which yields 1 point if the decision maker found it

## IBL Model for Gridworld Environment

::: nonincremental
-   Delayed feedback as a result of sequence of choices as opposed to a single choice
:::

<div>

```{python}
#| eval: false
agent.equal_delay_feedback(reward, episode_history)
```

</div>

## Ms. Pacman from GymAI

<div>

<div>

::: nonincremental
-   Call Ms.Pacman environment using gym
:::

</div>

```{python}
#| eval: false

import gym

env = gym.make('MsPacman-v0')
```

</div>

## Discussion

We would be happy to hear your feedback on this workshop. What worked well, and what could we improve upon?

## Thank you!

DDMLab: [www.cmu.edu/ddmlab](www.cmu.edu/ddmlab)

Interested in building IBL models?

::: nonincremental
-   SpeedyIBL: [github.com/DDM-Lab/SpeedyIBL](https://github.com/DDM-Lab/SpeedyIBL){.uri}

-   PyIBL: [www.cmu.edu/dietrich/sds/ddmlab/downloads.html#py-ibl](https://www.cmu.edu/dietrich/sds/ddmlab/downloads.html#py-ibl){.uri}
:::

## References

### References
