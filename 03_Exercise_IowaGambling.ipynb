{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erinbugbee/2023CLIHC-SpeedyIBL-Workshop/blob/main/03_Exercise_IowaGambling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5CxWTj07Qz4"
      },
      "source": [
        "# Exercise: Using SpeedyIBL for the Iowa Gambling Task\n",
        "\n",
        "In the original paper (Bechara et al., 1994), the following procedure was followed:\n",
        "\n",
        "There were 4 decks of cards (A, B, C, and D).\n",
        "\n",
        "Participants had to choose in total 100 cards, one at the time.\n",
        "\n",
        "Each time they choose a card, they get feedback about winning and/or losing some money.\n",
        "\n",
        "Participants did not know what each card would yield in advance (i.e., like a lottery).\n",
        "\n",
        "Participants started with a \"loan\" of \\$2000 and were told to make a profit.\n",
        "\n",
        "Decks A and B always yielded \\$100.\n",
        "\n",
        "Decks C and D always yielded \\$50.\n",
        "\n",
        "For each card chosen, there is a 50\\% chance of having to pay a penalty as well. \n",
        "\n",
        "For decks A and B, the penalty is \\$250.\n",
        "\n",
        "For decks C and D, the penalty is \\$50.\n",
        "\n",
        "Learn more about the task here: https://www.psytoolkit.org/experiment-library/igt.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTy3tPMk7Qz6"
      },
      "outputs": [],
      "source": [
        "# TODO: Install speedyibl\n",
        "\n",
        "# TODO: Import speedyibl\n",
        "\n",
        "# TODO: Define an agent\n",
        "\n",
        "# TODO: Define the options\n",
        "\n",
        "import random\n",
        "# Define a reward function\n",
        "def reward(choice):\n",
        "    # Choice A or B\n",
        "    if choice == \"A\" or choice == \"B\":\n",
        "        r = 100\n",
        "        if random.random() <= 0.5:\n",
        "            r -= 250\n",
        "    # Choice C or D\n",
        "    else:\n",
        "        r = 50\n",
        "        if random.random() <= 0.5:\n",
        "            r -= 50\n",
        "    return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT_mC4IO7Qz7"
      },
      "outputs": [],
      "source": [
        "# Run experiments\n",
        "import time # to calculate time\n",
        "runs = 1000 # number of runs (participants)\n",
        "trials = 100 # number of trials (episodes)\n",
        "\n",
        "\n",
        "def run(agent, reward, n_runs, trials):\n",
        "  average_p = [] # to store average of performance (proportion of maximum reward expectation choice)\n",
        "  average_r = []\n",
        "  average_time = [] # to save time\n",
        "  for r in range(n_runs):\n",
        "    pmax = []\n",
        "    rewards = []\n",
        "    ttime = [0]\n",
        "    agent.reset() #clear the memory for a new run\n",
        "    for i in range(trials):\n",
        "      start = time.time()\n",
        "      # TODO: Agent chooses one option from the list of four options\n",
        "\n",
        "      # determine the reward that agent can receive\n",
        "      re = reward(choice)\n",
        "      # TODO: Store the observed reward to memory of the agent\n",
        "\n",
        "      end = time.time()\n",
        "      ttime.append(ttime[-1] + end - start)\n",
        "      pmax.append(choice == \"C\" or choice == \"D\")\n",
        "      rewards.append(re)\n",
        "    average_p.append(pmax) # save performance of each run\n",
        "    average_r.append(rewards)\n",
        "    average_time.append(ttime) # save time of each run\n",
        "  return average_r, average_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run for 1000 runs and 100 trials with defined reward function\n",
        "average_r, average_p = run(agent, reward, runs, trials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz-rT9Kd7Qz7"
      },
      "outputs": [],
      "source": [
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Plot PMAX over rounds, which is the proportion of choices that are the options with the maximum reward expectation (Choice C or D)\n",
        "plt.plot(range(trials), np.mean(np.asarray(average_p), axis=0), \"o-\", color = \"darkgreen\", markersize=2, linestyle = \"--\", label = \"speedyIBL\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"PMAX\")\n",
        "plt.title(\"Performance\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pwT2Bcq7Qz8"
      },
      "outputs": [],
      "source": [
        "# Plot Average Reward over rounds\n",
        "plt.plot(range(trials), np.mean(np.asarray(average_r), axis=0), \"o-\", color = \"darkgreen\", markersize = 2, linestyle = \"--\", label = \"speedyIBL\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.title(\"Performance\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize agent with default utility of 10 instead of 110\n",
        "agent = Agent(default_utility=10)\n",
        "average_r, average_p = run(agent, reward, runs, trials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOYYbsdS7Qz8"
      },
      "outputs": [],
      "source": [
        "# Plot PMAX over rounds with default utility 10\n",
        "plt.plot(range(trials), np.mean(np.asarray(average_p), axis=0), \"o-\", color = \"darkgreen\", markersize=2, linestyle = \"--\", label = \"speedyIBL\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"PMAX\")\n",
        "plt.title(\"Performance\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKwz18CJ7Qz8"
      },
      "outputs": [],
      "source": [
        "# Plot Average Reward over rounds with default utility 10\n",
        "plt.plot(range(trials), np.mean(np.asarray(average_r), axis=0), \"o-\", color = \"darkgreen\", markersize = 2, linestyle = \"--\", label = \"speedyIBL\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.title(\"Performance\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Vary the noise (default = 0.25) and decay (default = 0.5) parameters and plot the PMAX and Average Reward over rounds with default utility 110\n",
        "\n",
        "\n",
        "# TODO: Run the simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Plot PMAX over rounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Plot Average Reward over rounds"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
